{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EP3.2_MAC5768.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "4f8763616d36299a7e67065bbff61b4594b946871d4c1b06c8acd4b8a7e0d76f"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MunxHCupm-P"
      },
      "source": [
        "# EP 3.2 -  Visão e Processamento de Imagens\n",
        "\n",
        "###Integrantes do grupo:\n",
        "\n",
        "-> Ciro Akiyoshi Higashi - nusp:10736858\n",
        "\n",
        "-> Lucas Giannella de Oliveira - nusp:10336021\n",
        "\n",
        "-> Link do Git: https://github.com/Cirokun/MAC0417-MAC5768\n",
        "\n",
        "-> Link para o Google Drive: https://drive.google.com/drive/folders/18tiGKyk2206Kd_v0qlBazA4Hcn5zeODi?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye8NM2O0qKa2"
      },
      "source": [
        "## Importação de bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fco17wC2p0rq"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn import neural_network,naive_bayes\n",
        "from skimage import io, color, exposure, filters, transform,measure,morphology\n",
        "from sklearn import decomposition,model_selection\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyOM_tGuRE3p"
      },
      "source": [
        "#Conexão com o Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr5hrx0Q4zET",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51442137-2d67-4bd0-809b-5952a4a69859"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dK0nzsGRFO1"
      },
      "source": [
        "## Mudança para o diretorio de trabalho"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqDvuqOdQuKq"
      },
      "source": [
        "os.chdir('/content/drive/MyDrive/MAC5768/EPs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A80Afq5GRD20"
      },
      "source": [
        "## Função para recolher os dados\n",
        "-> A função a seguir recebe dois vetores (Entrada e Respostas) e um diretorio\n",
        "\n",
        "-> Do diretorio lê todas as imagens e as coloca no vetor de entrada ($\\textbf{X}$)\n",
        "\n",
        "-> A partir dos metadados no nome da imagem cria-se o vetor das respostas ($\\textbf{Y}$)\n",
        "\n",
        "Obs: Durantes os testes iniciais colocar todas as imagens nos vetores estava extrapolando a RAM maxima do ambiente de execução, portanto optamos por utilizar versões redimensionadas (reduzidas) das imagens segmentas. \n",
        "\n",
        "Esta reduçao foi de 1/8 diminuindo as imagens de 4000x2248 para 500x281.\n",
        "As imagens redimensionadas foram inspecionadas manualmente para verificar que não havia ocorrido perda de informações significativas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fwblo5SQkc1"
      },
      "source": [
        "def loadXY(pathMan,X,Y):\n",
        "    for filename in os.listdir(pathMan):\n",
        "        img = io.imread(os.path.join(pathMan,filename), as_gray=True)\n",
        "        classe=(filename.split(\"_\")[0])[:-1]\n",
        "        if img is not None:\n",
        "            X = np.append(X, np.array(img).reshape((1,X.shape[1])), axis=0)\n",
        "            if (classe == \"Caneca\") :\n",
        "                Y=np.append(Y, np.array([[0]]), axis=0)\n",
        "            elif (classe == \"Chave\") :\n",
        "                Y=np.append(Y, np.array([[1]]), axis=0)\n",
        "            elif (classe == \"Escova\") :\n",
        "                Y=np.append(Y, np.array([[2]]), axis=0)\n",
        "            elif (classe == \"Dado\") :\n",
        "                Y=np.append(Y, np.array([[3]]), axis=0)\n",
        "            elif (classe == \"Meia\") :\n",
        "                Y=np.append(Y, np.array([[4]]), axis=0)\n",
        "            elif (classe == \"Fidget\") :\n",
        "                Y=np.append(Y, np.array([[5]]), axis=0)\n",
        "            elif (classe == \"Laranja\") :\n",
        "                Y=np.append(Y, np.array([[6]]), axis=0)\n",
        "            elif (classe == \"Garfo\") :\n",
        "                Y=np.append(Y, np.array([[7]]), axis=0)\n",
        "            elif (classe == \"Moeda\") :\n",
        "                Y=np.append(Y, np.array([[8]]), axis=0)\n",
        "            elif (classe == \"Prato\") :\n",
        "                Y=np.append(Y, np.array([[9]]), axis=0)\n",
        "    return X,Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrr0q2rFS6hg"
      },
      "source": [
        "Criação dos vetores entrada a respostas e fetch das imagens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_7UezOMQkc2",
        "outputId": "9e2ddcac-9199-4f99-efaf-5767477d1dd4"
      },
      "source": [
        "X = np.empty((0,140500),int)\n",
        "Y=np.empty((0,1),int)\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "X,Y=loadXY(\"SegRe\",X,Y)\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 140500)\n",
            "(0, 1)\n",
            "(5760, 140500)\n",
            "(5760, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA_wSmdRTH3T"
      },
      "source": [
        "## Separação dos dados em conjunto de teste e treino\n",
        "\n",
        "Aqui separamos nosso conjunto de dados em treino e teste. A proporção utilizado foi de 70% para treino e os 30% restantes para teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XdGxp-0Qkc2",
        "outputId": "d17ce201-35c0-46ba-8a42-32d436d15cb0"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.3, shuffle=True)\n",
        "print(f\"X_train dims={X_train.shape}\")\n",
        "print(f\"Y_train dims={Y_train.shape}\")\n",
        "print(f\"X_test dims={X_test.shape}\")\n",
        "print(f\"Y_test dims={Y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train dims=(4032, 140500)\n",
            "Y_train dims=(4032, 1)\n",
            "X_test dims=(1728, 140500)\n",
            "Y_test dims=(1728, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3vnnofDTVPb"
      },
      "source": [
        "## Redução de dimensionalidade via decomposição em valores singulares\n",
        "\n",
        "Aqui utilizamos o algoritmo de SVD para reduzir ainda mais a dimensionalidade dos nossos dados de forma a manter a variância deles conservadas. Reduzimos o número de features para 150 e conseguimos manter 86% da variância total com essas features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biKF5yC8Qkc3",
        "outputId": "6e5601e7-c938-4536-bf6f-cd64960ecae1"
      },
      "source": [
        "n_comp = 150\n",
        "svd = decomposition.TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
        "svd.fit(X_train.astype(np.float32))\n",
        "print(f\"Percentual da variancia explicada {svd.explained_variance_ratio_.sum()}\")\n",
        "\n",
        "train_features = svd.transform(X_train.astype(np.float32))\n",
        "test_features = svd.transform(X_test.astype(np.float32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentual da variancia explicada 0.8633547425270081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhVTNayqTgXR"
      },
      "source": [
        "## Treino do classificador de vetores suporte \n",
        "\n",
        "Aqui fazemos um tuning para os hiperparâmetros do  modelo de SVM para classificação.| "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL8bIP0Vz_-p",
        "outputId": "c8c59a8c-ae04-4953-8397-b1bde416808a"
      },
      "source": [
        "param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\n",
        "clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
        "clf = clf.fit(train_features, Y_train.ravel())\n",
        "print(clf.best_estimator_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVC(C=1000.0, class_weight='balanced', gamma=0.0001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWIBVI-STzrI"
      },
      "source": [
        "## Predição e relatorio para o SVC\n",
        "\n",
        "Ao ajustarmos o modelo de SCV chegamos em uma métrica de acurácia de 0.23, o que é bem pequeno. Ao analisarmos melhor nossa matriz de confusão chegamos à conclusão de que o SVC em geral classifica todas as imagens como canecas e pratos, não conseguimos chegar a um motivo específico para esse comportamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LIz6YfXvIQL",
        "outputId": "a29baeb3-29fe-46f1-fb17-459d6859b962"
      },
      "source": [
        "SVM_pred = clf.predict(test_features)\n",
        "target_names=[\"Caneca\",\"Chave\",\"Escova\",\"Dado\",\"Meia\",\"Fidget\",\"Laranja\",\"Garfo\",\"Moeda\",\"Prato\"]\n",
        "n_classes=len(target_names)\n",
        "print(\"Relatorio para SVM\")\n",
        "print(classification_report(Y_test, SVM_pred, target_names=target_names))\n",
        "print(confusion_matrix(Y_test, SVM_pred, labels=range(n_classes)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Relatorio para SVM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Caneca       0.13      0.91      0.23       179\n",
            "       Chave       1.00      0.26      0.41       165\n",
            "      Escova       1.00      0.06      0.11       168\n",
            "        Dado       1.00      0.07      0.14       163\n",
            "        Meia       1.00      0.09      0.16       176\n",
            "      Fidget       0.95      0.12      0.22       170\n",
            "     Laranja       1.00      0.13      0.23       179\n",
            "       Garfo       1.00      0.02      0.04       175\n",
            "       Moeda       1.00      0.22      0.36       174\n",
            "       Prato       0.22      0.41      0.28       179\n",
            "\n",
            "    accuracy                           0.23      1728\n",
            "   macro avg       0.83      0.23      0.22      1728\n",
            "weighted avg       0.82      0.23      0.22      1728\n",
            "\n",
            "[[163   0   0   0   0   0   0   0   0  16]\n",
            " [ 95  43   0   0   0   1   0   0   0  26]\n",
            " [131   0  10   0   0   0   0   0   0  27]\n",
            " [112   0   0  12   0   0   0   0   0  39]\n",
            " [133   0   0   0  15   0   0   0   0  28]\n",
            " [134   0   0   0   0  21   0   0   0  15]\n",
            " [118   0   0   0   0   0  23   0   0  38]\n",
            " [129   0   0   0   0   0   0   4   0  42]\n",
            " [ 99   0   0   0   0   0   0   0  38  37]\n",
            " [105   0   0   0   0   0   0   0   0  74]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0TKdQrLT089"
      },
      "source": [
        "## Teste adiconal utilizando Multi Layer Perceptron (MLP)\n",
        "\n",
        "Aqui ajustamos a rede neural MLP e, com a mesma métrica de acurácia, vemos que esse algoritmo obtém acurácia bem mais satisfatória de 0.73. Com a análise da matriz de confusão, vemos que o problema de classificações das canecas não acontece com esse algoritmo, apesar de que ainda conseguimos notar um número maior de classificações em pratos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLPU0lUaQkc6",
        "outputId": "3575cddd-e611-450f-a15e-cdc957109405"
      },
      "source": [
        "MLP=neural_network.MLPClassifier(max_iter=1000)\n",
        "MLP.fit(train_features, Y_train.ravel())\n",
        "MLP_Score=MLP.score(test_features, Y_test)\n",
        "MLP_Pred=MLP.predict(test_features)\n",
        "print(\"Relatorio para MLP\")\n",
        "print(classification_report(Y_test, MLP_Pred, target_names=target_names))\n",
        "print(confusion_matrix(Y_test, MLP_Pred, labels=range(n_classes)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Relatorio para MLP\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Caneca       0.85      0.78      0.82       179\n",
            "       Chave       0.85      0.73      0.78       165\n",
            "      Escova       0.82      0.77      0.80       168\n",
            "        Dado       0.76      0.60      0.67       163\n",
            "        Meia       0.92      0.74      0.82       176\n",
            "      Fidget       0.90      0.78      0.84       170\n",
            "     Laranja       0.86      0.63      0.73       179\n",
            "       Garfo       0.91      0.61      0.73       175\n",
            "       Moeda       0.83      0.72      0.78       174\n",
            "       Prato       0.39      0.97      0.55       179\n",
            "\n",
            "    accuracy                           0.73      1728\n",
            "   macro avg       0.81      0.73      0.75      1728\n",
            "weighted avg       0.81      0.73      0.75      1728\n",
            "\n",
            "[[140   3   2   5   5   4   0   0   2  18]\n",
            " [  3 120   1   9   0   3   1   1   1  26]\n",
            " [  1   0 130   1   1   0   0   4   1  30]\n",
            " [  3   5   4  97   2   1   1   0  11  39]\n",
            " [  5   5   0   1 130   3   1   1   2  28]\n",
            " [  6   4   1   4   2 133   4   0   0  16]\n",
            " [  4   4   5   2   0   3 113   4   4  40]\n",
            " [  1   0  11   6   1   0   4 106   4  42]\n",
            " [  1   1   4   2   0   0   3   0 126  37]\n",
            " [  0   0   0   1   1   0   4   0   0 173]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GFG_NXBT7fe"
      },
      "source": [
        "## Teste adiconal utilizando o classificador Naive Bayes\n",
        "\n",
        "Ajustaremos por fim um modelo de Naive Bayes. Chegamos a uma acurácia de 0.33. E quanto a matriz de confusão, vemos que está possui a mesma característica de classificar muitos objetos em pratos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrpRjq3dQkc6",
        "outputId": "f55ad8c8-a8ae-48f6-992e-e9f1800c47ec"
      },
      "source": [
        "NB=naive_bayes.GaussianNB()\n",
        "NB.fit(train_features, Y_train.ravel())\n",
        "NB_Score=NB.score(test_features, Y_test)\n",
        "NB_Pred=NB.predict(test_features)\n",
        "print(\"Relatorio para Naive Bayes\")\n",
        "print(classification_report(Y_test, NB_Pred, target_names=target_names))\n",
        "print(confusion_matrix(Y_test, NB_Pred, labels=range(n_classes)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Relatorio para Naive Bayes\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Caneca       0.70      0.41      0.52       179\n",
            "       Chave       0.49      0.27      0.35       165\n",
            "      Escova       0.67      0.28      0.39       168\n",
            "        Dado       0.42      0.05      0.09       163\n",
            "        Meia       0.48      0.36      0.41       176\n",
            "      Fidget       0.65      0.46      0.54       170\n",
            "     Laranja       0.40      0.18      0.25       179\n",
            "       Garfo       0.79      0.31      0.44       175\n",
            "       Moeda       0.70      0.13      0.22       174\n",
            "       Prato       0.15      0.83      0.25       179\n",
            "\n",
            "    accuracy                           0.33      1728\n",
            "   macro avg       0.54      0.33      0.35      1728\n",
            "weighted avg       0.54      0.33      0.35      1728\n",
            "\n",
            "[[ 74  22   0   4   7  22   4   0   0  46]\n",
            " [  3  44   0   0   5  10   7   0   4  92]\n",
            " [  4   1  47   0   7   0  10   4   2  93]\n",
            " [  4  12   0   8   4   0   3   0   1 131]\n",
            " [  9   4   2   0  63   1   4   0   0  93]\n",
            " [  9   2   0   2   3  79   2   2   0  71]\n",
            " [  1   0   0   0   6   8  32   3   3 126]\n",
            " [  1   1  16   4   2   2   8  54   0  87]\n",
            " [  0   4   5   1   4   0  11   5  23 121]\n",
            " [  0   0   0   0  31   0   0   0   0 148]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tm-md84IcLv"
      },
      "source": [
        "## Conclusões\n",
        "\n",
        "Chegamos que o modelo melhor ajustado foi o de Multi Layer Perceptron (MLP), com uma acurácia de 0.73. \n",
        "\n",
        "Algumas hipóteses que tínhamos de que ,por exemplo, nosso algoritmo fosse confundir as moedas com os dados, pois ambos tem formatos e tamanhos similares não aconteceu. \n",
        "\n",
        "Porém, um problema que surgiu e pareceu constante nos três modelos ajustados foi o fato de que em geral, tende-se a classificar os objetos em pratos, ou seja, quase todas nossas classificações erradas foram classificadas como pratos. Não conseguimos chegar a um motivo específico para tal comportamento, mas vale ressaltar que as imagens dos pratos em geral eram as que ocupavam maior área da imagem. "
      ]
    }
  ]
}